{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b050304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "DATA_PATH = kagglehub.dataset_download(\"theseus200719/math-equations-dataset-aidav7-modified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d47e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fb0b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгоняем изображения под общий размер, сохраняя отношение сторон. Паддим справа.\n",
    "def resize_and_pad(img, target_height=64, target_width=256):\n",
    "    h, w = img.shape\n",
    "    scale = target_height / h\n",
    "    new_w = int(w * scale)\n",
    "\n",
    "    if new_w > target_width:\n",
    "        new_w = target_width\n",
    "\n",
    "    resized = cv2.resize(img, (new_w, target_height))\n",
    "\n",
    "    padded = np.zeros((target_height, target_width), dtype=np.float32)\n",
    "    padded[:, :new_w] = resized\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d356d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Dataset =====\n",
    "class FormulaDataset(Dataset):\n",
    "    def __init__(self, path, vocab, transform=None):\n",
    "        self.data = pd.read_csv(os.path.join(path, 'annotations.csv'))\n",
    "        self.img_dir = os.path.join(path, 'images')\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # --- картинка ---\n",
    "        img_path = os.path.join(self.img_dir, row['filenames'])\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = resize_and_pad(img)  # ATTENTION!!!\n",
    "        img = img / 255.0 # нормируем\n",
    "        img = torch.tensor(img).unsqueeze(0).float()  # (1,H,W)\n",
    "\n",
    "        # --- токены ---\n",
    "        info = ast.literal_eval(row['image_data'])\n",
    "        tokens = info['full_latex_chars']   # список строк-токенов\n",
    "        token_ids = self.vocab.encode(tokens)\n",
    "\n",
    "        return img, torch.tensor(token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfae95f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Vocabulary =====\n",
    "class Vocab:\n",
    "    def __init__(self, token_list):\n",
    "\n",
    "        # спец символы\n",
    "        self.pad = \"<pad>\"\n",
    "        self.bos = \"<bos>\"\n",
    "        self.eos = \"<eos>\"\n",
    "        self.unk = \"<unk>\"\n",
    "\n",
    "        self.tokens = [self.pad, self.bos, self.eos, self.unk] + sorted(token_list)\n",
    "        self.stoi = {t: i for i, t in enumerate(self.tokens)}\n",
    "        self.itos = {i: t for t, i in self.stoi.items()}\n",
    "\n",
    "        self.length = len(self.tokens)  # ATTENTION!!!\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def encode(self, token_seq):\n",
    "        if not isinstance(token_seq, list):\n",
    "            raise ValueError(f\"Expected list of tokens, got {type(token_seq)}\")\n",
    "        \n",
    "        if not all(isinstance(t, str) for t in token_seq):\n",
    "            raise ValueError(\"All tokens must be strings\")\n",
    "        \n",
    "        return [self.stoi[self.bos]] + \\\n",
    "               [self.stoi.get(t, self.stoi[self.unk]) for t in token_seq] + \\\n",
    "               [self.stoi[self.eos]]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # Декодируем и обрезаем по первому <eos>.\n",
    "        eos_id = self.stoi[self.eos]\n",
    "        pad_id = self.stoi[self.pad]\n",
    "        bos_id = self.stoi[self.bos]\n",
    "\n",
    "        toks = []\n",
    "        for i in ids:\n",
    "            if i == eos_id:\n",
    "                break\n",
    "            # Пропускаем pad и bos токены\n",
    "            if i not in (pad_id, bos_id):\n",
    "                toks.append(self.itos[i])\n",
    "\n",
    "        return \"\".join(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3442fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:06<00:00, 1502.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Ищем уникальные токены из всего списка токенов\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, \"annotations.csv\"))\n",
    "\n",
    "all_token_lists = []\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    info = ast.literal_eval(row['image_data'])\n",
    "    tokens = info['full_latex_chars']\n",
    "    all_token_lists.append(tokens)\n",
    "\n",
    "unique_tokens = set()\n",
    "for seq in all_token_lists:\n",
    "    unique_tokens.update(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d62c3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokens.json\", \"w\") as f:\n",
    "    json.dump(list(unique_tokens), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "250075df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<bos>', '<eos>', '<unk>', '+', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', '[', '\\\\cdot', '\\\\cos', '\\\\cot', '\\\\csc', '\\\\frac', '\\\\infty', '\\\\left(', '\\\\left|', '\\\\lim_', '\\\\ln', '\\\\log', '\\\\pi', '\\\\right)', '\\\\right|', '\\\\sec', '\\\\sin', '\\\\sqrt', '\\\\tan', '\\\\theta', '\\\\to', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'g', 'h', 'k', 'n', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}']\n"
     ]
    }
   ],
   "source": [
    "with open(\"tokens.json\", \"r\") as f:\n",
    "    token_list = json.load(f)\n",
    "\n",
    "vocab = Vocab(token_list)\n",
    "print(vocab.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b824e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs, seqs = zip(*batch)\n",
    "\n",
    "    # картинки: можно просто в stack\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "\n",
    "    # токены: паддим до максимальной длины в батче\n",
    "    max_len = max(len(seq) for seq in seqs)\n",
    "    padded_seqs = torch.full((len(seqs), max_len), fill_value=vocab.stoi[vocab.pad], dtype=torch.long)\n",
    "\n",
    "    for i, seq in enumerate(seqs):\n",
    "        padded_seqs[i, :len(seq)] = seq\n",
    "\n",
    "    return imgs, padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "546ed03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём датасет\n",
    "dataset = FormulaDataset(DATA_PATH, vocab)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, pin_memory=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dec82b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CNN encoder ---\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(128, hidden_dim, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.conv(x)  # [B,Hid,h,w]\n",
    "        B, C, H, W = feats.shape\n",
    "        # Преобразуем в последовательность: [B, H*W, C]\n",
    "        feats = feats.permute(0, 2, 3, 1).reshape(B, H*W, C)\n",
    "        return feats  # [B, seq_len, hidden_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cdf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=256, num_layers=4, nhead=8, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.pos_emb = nn.Embedding(max_len, hidden_dim)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        B, T = tgt.shape\n",
    "        tok_emb = self.token_emb(tgt)  # [B,T,H]\n",
    "        pos = self.pos_emb(torch.arange(T, device=tgt.device))  # [T,H]\n",
    "        pos = pos.unsqueeze(0).expand(B, -1, -1)\n",
    "        tgt_emb = tok_emb + pos\n",
    "\n",
    "        # маска [T,T]\n",
    "        tgt_mask = torch.triu(torch.ones(T, T, device = tgt.device), diagonal=1).bool()\n",
    "\n",
    "        out = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask)  # [B,T,H]\n",
    "        return self.fc_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b703c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Итоговая модель ---\n",
    "class FormulaRecognizer(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=256, max_len=512):\n",
    "        super().__init__()\n",
    "        self.encoder = CNNEncoder(hidden_dim)\n",
    "        self.decoder = TransformerDecoder(vocab_size, hidden_dim, max_len=max_len)\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, images, tokens):\n",
    "        memory = self.encoder(images)  # [B,S,H]\n",
    "        out = self.decoder(tokens[:, :-1], memory)  # предсказываем без последнего токена\n",
    "        return out  # [B,T-1,V]\n",
    "\n",
    "    def greedy_decode(self, image, start_token, end_token, device=\"cpu\"):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            memory = self.encoder(image.unsqueeze(0).to(device))  # [1,S,H]\n",
    "\n",
    "            tokens = torch.tensor([[start_token]], device=device)\n",
    "            for _ in range(self.max_len):\n",
    "                out = self.decoder(tokens, memory)  # [1,T,V]\n",
    "                next_token = out[:, -1, :].argmax(-1)  # [1]\n",
    "                tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)\n",
    "                if next_token.item() == end_token:\n",
    "                    break\n",
    "\n",
    "        return tokens.squeeze(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40a8b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, tokenizer, epochs=5, device=\"cuda\"):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # паддинг\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ---------- TRAIN ----------\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "\n",
    "        for imgs, tokens in train_bar:\n",
    "            imgs, tokens = imgs.to(device), tokens.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs, tokens[:, :-1])  # [B, T-1, vocab_size]\n",
    "\n",
    "            # делаем срез по минимальной длине\n",
    "            min_len = min(outputs.size(1), tokens[:, 1:].size(1))\n",
    "            outputs = outputs[:, :min_len, :]\n",
    "            targets = tokens[:, 1:1+min_len]\n",
    "\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, outputs.size(-1)),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # ---------- VALID ----------\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        examples = []\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "            for imgs, tokens in val_bar:\n",
    "                imgs, tokens = imgs.to(device), tokens.to(device)\n",
    "                outputs = model(imgs, tokens[:, :-1])\n",
    "\n",
    "                min_len = min(outputs.size(1), tokens[:, 1:].size(1))\n",
    "                outputs = outputs[:, :min_len, :]\n",
    "                targets = tokens[:, 1:1+min_len]\n",
    "\n",
    "                loss = criterion(\n",
    "                    outputs.reshape(-1, outputs.size(-1)),\n",
    "                    targets.reshape(-1)\n",
    "                )\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                if len(examples) < 3:\n",
    "                    preds = outputs.argmax(-1)  # [B, T]\n",
    "                    pred_text = tokenizer.decode([t for t in preds[0].cpu().tolist() if t != 0])\n",
    "                    true_text = tokenizer.decode([t for t in tokens[0].cpu().tolist() if t != 0])\n",
    "                    examples.append((pred_text, true_text))\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        for i, (pred, true) in enumerate(examples):\n",
    "            print(f\"  EX{i+1}: pred = {pred}\")\n",
    "            print(f\"       true = {true}\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"model_epoch{epoch+1}.pth\")\n",
    "        print(f\"Model saved: model_epoch{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5983f17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   1%|          | 15/1250 [00:07<10:50,  1.90it/s, loss=2.38]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m FormulaRecognizer(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vocab))\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, tokenizer, epochs, device)\u001b[0m\n\u001b[0;32m     10\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     11\u001b[0m train_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Train]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ivan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Ivan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Ivan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Ivan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Ivan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m, in \u001b[0;36mFormulaDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# --- картинка ---\u001b[39;00m\n\u001b[0;32m     16\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilenames\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 17\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMREAD_GRAYSCALE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m img \u001b[38;5;241m=\u001b[39m resize_and_pad(img)  \u001b[38;5;66;03m# ATTENTION!!!\u001b[39;00m\n\u001b[0;32m     19\u001b[0m img \u001b[38;5;241m=\u001b[39m img \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m \u001b[38;5;66;03m# нормируем\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = FormulaRecognizer(vocab_size=len(vocab))\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=vocab,\n",
    "    epochs=10,\n",
    "    device=device\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
