{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b050304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "DATA_PATH = kagglehub.dataset_download(\"theseus200719/math-equations-dataset-aidav7-modified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d47e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb0b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгоняем изображения под общий размер, сохраняя отношение сторон. Паддим справа.\n",
    "def resize_and_pad(img, target_height=64, target_width=256):\n",
    "    h, w = img.shape\n",
    "    scale = target_height / h\n",
    "    new_w = int(w * scale)\n",
    "\n",
    "    if new_w > target_width:\n",
    "        new_w = target_width\n",
    "\n",
    "    resized = cv2.resize(img, (new_w, target_height))\n",
    "\n",
    "    padded = np.zeros((target_height, target_width), dtype=np.float32)\n",
    "    padded[:, :new_w] = resized\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d356d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Dataset =====\n",
    "class FormulaDataset(Dataset):\n",
    "    def __init__(self, path, vocab, transform=None):\n",
    "        self.data = pd.read_csv(os.path.join(path, 'annotations.csv'))\n",
    "        self.img_dir = os.path.join(path, 'images')\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # --- картинка ---\n",
    "        img_path = os.path.join(self.img_dir, row['filenames'])\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = resize_and_pad(img)  # ATTENTION!!!\n",
    "        img = img / 255.0 # нормируем\n",
    "        img = torch.tensor(img).unsqueeze(0).float()  # (1,H,W)\n",
    "\n",
    "        # --- токены ---\n",
    "        info = ast.literal_eval(row['image_data'])\n",
    "        tokens = info['full_latex_chars']   # список строк-токенов\n",
    "        token_ids = self.vocab.encode(tokens)\n",
    "\n",
    "        return img, torch.tensor(token_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae4042b",
   "metadata": {},
   "source": [
    "Добавить чтение файла vocab.json с токенами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfae95f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Vocabulary =====\n",
    "class Vocab:\n",
    "    def __init__(self, token_list):\n",
    "\n",
    "        # спец символы\n",
    "        self.pad = \"<pad>\"\n",
    "        self.bos = \"<bos>\"\n",
    "        self.eos = \"<eos>\"\n",
    "        self.unk = \"<unk>\"\n",
    "\n",
    "        self.tokens = [self.pad, self.bos, self.eos, self.unk] + sorted(token_list)\n",
    "        self.stoi = {t: i for i, t in enumerate(self.tokens)}\n",
    "        self.itos = {i: t for t, i in self.stoi.items()}\n",
    "\n",
    "        self.length = len(self.tokens)  # ATTENTION!!!\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def encode(self, token_seq):\n",
    "        # IMPROVED: Added input validation\n",
    "        if not isinstance(token_seq, list):\n",
    "            raise ValueError(f\"Expected list of tokens, got {type(token_seq)}\")\n",
    "        \n",
    "        if not all(isinstance(t, str) for t in token_seq):\n",
    "            raise ValueError(\"All tokens must be strings\")\n",
    "        \n",
    "        # Принимает список токенов.\n",
    "        return [self.stoi[self.bos]] + \\\n",
    "               [self.stoi.get(t, self.stoi[self.unk]) for t in token_seq] + \\\n",
    "               [self.stoi[self.eos]]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # Декодируем и обрезаем по первому <eos>.\n",
    "        eos_id = self.stoi[self.eos]\n",
    "        pad_id = self.stoi[self.pad]\n",
    "        bos_id = self.stoi[self.bos]\n",
    "\n",
    "        toks = []\n",
    "        for i in ids:\n",
    "            if i == eos_id:\n",
    "                break\n",
    "            if i not in (pad_id, bos_id):\n",
    "                toks.append(self.itos[i])\n",
    "\n",
    "        return \"\".join(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3442fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:12<00:00, 1388.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Ищем уникальные токены из всего списка токенов\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, \"annotations.csv\"))\n",
    "\n",
    "all_token_lists = []\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    info = ast.literal_eval(row['image_data'])\n",
    "    tokens = info['full_latex_chars']\n",
    "    all_token_lists.append(tokens)\n",
    "\n",
    "unique_tokens = set()\n",
    "for seq in all_token_lists:\n",
    "    unique_tokens.update(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62c3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"tokens.json\", \"w\") as f:\n",
    "    json.dump(list(unique_tokens), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250075df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<bos>', '<eos>', '<unk>', '+', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', '[', '\\\\cdot', '\\\\cos', '\\\\cot', '\\\\csc', '\\\\frac', '\\\\infty', '\\\\left(', '\\\\left|', '\\\\lim_', '\\\\ln', '\\\\log', '\\\\pi', '\\\\right)', '\\\\right|', '\\\\sec', '\\\\sin', '\\\\sqrt', '\\\\tan', '\\\\theta', '\\\\to', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'g', 'h', 'k', 'n', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}']\n"
     ]
    }
   ],
   "source": [
    "with open(\"tokens.json\", \"r\") as f:\n",
    "    token_list = json.load(f)\n",
    "\n",
    "vocab = Vocab(token_list)\n",
    "print(vocab.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b824e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs, seqs = zip(*batch)\n",
    "\n",
    "    # картинки: можно просто в stack\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "\n",
    "    # токены: паддим до максимальной длины в батче\n",
    "    max_len = max(len(seq) for seq in seqs)\n",
    "    padded_seqs = torch.full((len(seqs), max_len), fill_value=vocab.stoi[vocab.pad], dtype=torch.long)\n",
    "\n",
    "    for i, seq in enumerate(seqs):\n",
    "        padded_seqs[i, :len(seq)] = seq\n",
    "\n",
    "    return imgs, padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546ed03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# создаём датасет\n",
    "dataset = FormulaDataset(DATA_PATH, vocab)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, pin_memory=True, collate_fn=collate_fn, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, pin_memory=True, collate_fn=collate_fn, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea6c724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- CNN encoder (ResNet-like) ---\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(128, hidden_dim, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [B,1,H,W] → [B,Hid,h,w]\n",
    "        feats = self.conv(x)\n",
    "        B, C, H, W = feats.shape\n",
    "        # flatten → [B, HW, C]\n",
    "        feats = feats.permute(0, 2, 3, 1).reshape(B, H*W, C)\n",
    "        return feats  # [B, seq_len, hidden_dim]\n",
    "# --- Transformer decoder ---\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=256, num_layers=4, nhead=8, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.pos_emb = nn.Embedding(max_len, hidden_dim)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,  # теперь входы [B,T,H], а не [T,B,H]\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        \"\"\"\n",
    "        tgt: [B, T] токены\n",
    "        memory: [B, S, H] фичи энкодера\n",
    "        \"\"\"\n",
    "        B, T = tgt.shape\n",
    "        tok_emb = self.token_emb(tgt)\n",
    "        pos = self.pos_emb(torch.arange(T, device=tgt.device))  # [T,H]\n",
    "        pos = pos.unsqueeze(0).expand(B, -1, -1)\n",
    "        tgt_emb = tok_emb + pos  # [B,T,H]\n",
    "\n",
    "        # TransformerDecoder c batch_first=True → ждёт [B,T,H], memory [B,S,H]\n",
    "        out = self.transformer_decoder(tgt_emb, memory)  # [B,T,H]\n",
    "        return self.fc_out(out)  # [B,T,V]\n",
    "\n",
    "# --- Итоговая модель ---\n",
    "class FormulaRecognizer(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = CNNEncoder(hidden_dim)\n",
    "        self.decoder = TransformerDecoder(vocab_size, hidden_dim)\n",
    "\n",
    "    def forward(self, images, tokens):\n",
    "        memory = self.encoder(images)  # [B,S,H]\n",
    "        out = self.decoder(tokens, memory)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "\n",
    "def train_model(model, train_loader, val_loader, tokenizer, epochs=5, device=\"cuda\"):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.stoi[tokenizer.pad])  # ignore padding\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    \n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ---------- TRAIN ----------\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "\n",
    "        for imgs, tokens in train_bar:\n",
    "            imgs, tokens = imgs.to(device), tokens.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # FIXED: Proper teacher forcing - input is tokens[:-1], target is tokens[1:]\n",
    "            outputs = model(imgs, tokens[:, :-1])  # [B, T-1, vocab_size]\n",
    "            targets = tokens[:, 1:]  # [B, T-1] - shifted targets (no BOS, includes EOS)\n",
    "            \n",
    "            # FIXED: Ensure same length for outputs and targets\n",
    "            seq_len = min(outputs.size(1), targets.size(1))\n",
    "            outputs = outputs[:, :seq_len, :]  # [B, seq_len, vocab_size]\n",
    "            targets = targets[:, :seq_len]     # [B, seq_len]\n",
    "\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, outputs.size(-1)),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += targets.numel()\n",
    "            train_bar.set_postfix(loss=loss.item(), ppl=math.exp(loss.item()))\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_ppl = math.exp(avg_train_loss)\n",
    "\n",
    "        # ---------- VALID ----------\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_tokens = 0\n",
    "        examples = []\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "            for imgs, tokens in val_bar:\n",
    "                imgs, tokens = imgs.to(device), tokens.to(device)\n",
    "                \n",
    "                # FIXED: Same teacher forcing logic as training\n",
    "                outputs = model(imgs, tokens[:, :-1])\n",
    "                targets = tokens[:, 1:]\n",
    "                \n",
    "                seq_len = min(outputs.size(1), targets.size(1))\n",
    "                outputs = outputs[:, :seq_len, :]\n",
    "                targets = targets[:, :seq_len]\n",
    "\n",
    "                loss = criterion(\n",
    "                    outputs.reshape(-1, outputs.size(-1)),\n",
    "                    targets.reshape(-1)\n",
    "                )\n",
    "                val_loss += loss.item()\n",
    "                val_tokens += targets.numel()\n",
    "                \n",
    "                if len(examples) < 3:\n",
    "                    preds = outputs.argmax(-1)  # [B, seq_len]\n",
    "                    pred_text = tokenizer.decode(preds[0].cpu().tolist())\n",
    "                    true_text = tokenizer.decode(tokens[0].cpu().tolist())\n",
    "                    examples.append((pred_text, true_text))\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_ppl = math.exp(avg_val_loss)\n",
    "\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} (PPL: {train_ppl:.2f}) | Val Loss: {avg_val_loss:.4f} (PPL: {val_ppl:.2f})\")\n",
    "        for i, (pred, true) in enumerate(examples):\n",
    "            print(f\"  EX{i+1}: pred = {pred}\")\n",
    "            print(f\"       true = {true}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), os.path.join('models', 'best_model.pth'))\n",
    "            print(f\"New best model saved with val_loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save(model.state_dict(), os.path.join('models', f\"model_epoch{epoch+1}.pth\"))\n",
    "        print(f\"Checkpoint saved: model_epoch{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5983f17c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m FormulaRecognizer(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vocab))\n\u001b[0;32m      5\u001b[0m train_model(\n\u001b[0;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      7\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = FormulaRecognizer(vocab_size=len(vocab))\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=vocab,\n",
    "    epochs=10,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08dc554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd2ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
